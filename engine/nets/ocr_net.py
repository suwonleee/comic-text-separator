# GPL-3.0 â€” OCR recognition network (ConvNeXt + Transformer with XPOS)
import math
from typing import List, Optional
from collections import defaultdict

import torch
import torch.nn as nn
import torch.nn.functional as F
import einops

from .xpos import XPOS


class ConvNeXtBlock(nn.Module):
    def __init__(self, dim, layer_scale_init_value=1e-6, ks=7, padding=3):
        super().__init__()
        self.dwconv = nn.Conv2d(dim, dim, kernel_size=ks, padding=padding, groups=dim)
        self.norm = nn.BatchNorm2d(dim, eps=1e-6)
        self.pwconv1 = nn.Conv2d(dim, 4 * dim, 1, 1, 0)
        self.act = nn.GELU()
        self.pwconv2 = nn.Conv2d(4 * dim, dim, 1, 1, 0)
        self.gamma = (
            nn.Parameter(layer_scale_init_value * torch.ones(1, dim, 1, 1), requires_grad=True)
            if layer_scale_init_value > 0 else None
        )

    def forward(self, x):
        residual = x
        x = self.dwconv(x)
        x = self.norm(x)
        x = self.pwconv1(x)
        x = self.act(x)
        x = self.pwconv2(x)
        if self.gamma is not None:
            x = self.gamma * x
        return residual + x


class ConvNext_FeatureExtractor(nn.Module):
    def __init__(self, img_height=48, in_dim=3, dim=512, n_layers=12):
        super().__init__()
        base = dim // 8
        self.stem = nn.Sequential(
            nn.Conv2d(in_dim, base, 7, stride=1, padding=3),
            nn.BatchNorm2d(base), nn.ReLU(),
            nn.Conv2d(base, base * 2, 2, stride=2, padding=0),
            nn.BatchNorm2d(base * 2), nn.ReLU(),
            nn.Conv2d(base * 2, base * 2, 3, stride=1, padding=1),
            nn.BatchNorm2d(base * 2), nn.ReLU(),
        )
        self.block1 = self._make_layers(base * 2, 4)
        self.down1 = nn.Sequential(
            nn.Conv2d(base * 2, base * 4, 2, stride=2, padding=0),
            nn.BatchNorm2d(base * 4), nn.ReLU(),
        )
        self.block2 = self._make_layers(base * 4, 12)
        self.down2 = nn.Sequential(
            nn.Conv2d(base * 4, base * 8, (2, 1), stride=(2, 1), padding=(0, 0)),
            nn.BatchNorm2d(base * 8), nn.ReLU(),
        )
        self.block3 = self._make_layers(base * 8, 10, ks=5, padding=2)
        self.down3 = nn.Sequential(
            nn.Conv2d(base * 8, base * 8, (2, 1), stride=(2, 1), padding=(0, 0)),
            nn.BatchNorm2d(base * 8), nn.ReLU(),
        )
        self.block4 = self._make_layers(base * 8, 8, ks=3, padding=1)
        self.down4 = nn.Sequential(
            nn.Conv2d(base * 8, base * 8, (3, 1), stride=(1, 1), padding=(0, 0)),
            nn.BatchNorm2d(base * 8), nn.ReLU(),
        )

    @staticmethod
    def _make_layers(dim, n, ks=7, padding=3):
        return nn.Sequential(*[ConvNeXtBlock(dim, ks=ks, padding=padding) for _ in range(n)])

    def forward(self, x):
        x = self.stem(x)
        x = self.block1(x)
        x = self.down1(x)
        x = self.block2(x)
        x = self.down2(x)
        x = self.block3(x)
        x = self.down3(x)
        x = self.block4(x)
        x = self.down4(x)
        return x


def _transformer_encoder_forward(self, src, src_mask=None, src_key_padding_mask=None, is_causal=False):
    x = src
    if self.norm_first:
        x = x + self._sa_block(self.norm1(x), src_mask, src_key_padding_mask)
        x = x + self._ff_block(self.norm2(x))
    else:
        x = self.norm1(x + self._sa_block(x, src_mask, src_key_padding_mask))
        x = self.norm2(x + self._ff_block(x))
    return x


class XposMultiheadAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, self_attention=False, encoder_decoder_attention=False):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.head_dim = embed_dim // num_heads
        self.scaling = self.head_dim ** -0.5
        self.self_attention = self_attention
        self.encoder_decoder_attention = encoder_decoder_attention
        assert self.self_attention ^ self.encoder_decoder_attention
        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=True)
        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=True)
        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=True)
        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=True)
        self.xpos = XPOS(self.head_dim, embed_dim)
        self.batch_first = True
        self._qkv_same_embed_dim = True

    def forward(self, query, key, value, key_padding_mask=None, attn_mask=None,
                need_weights=False, is_causal=False, k_offset=0, q_offset=0):
        bsz, tgt_len, embed_dim = query.size()
        src_len = key.size(1)
        q = self.q_proj(query) * self.scaling
        k = self.k_proj(key)
        v = self.v_proj(value)
        q = q.view(bsz, tgt_len, self.num_heads, self.head_dim).transpose(1, 2).reshape(bsz * self.num_heads, tgt_len, self.head_dim)
        k = k.view(bsz, src_len, self.num_heads, self.head_dim).transpose(1, 2).reshape(bsz * self.num_heads, src_len, self.head_dim)
        v = v.view(bsz, src_len, self.num_heads, self.head_dim).transpose(1, 2).reshape(bsz * self.num_heads, src_len, self.head_dim)
        if self.xpos is not None:
            k = self.xpos(k, offset=k_offset, downscale=True)
            q = self.xpos(q, offset=q_offset, downscale=False)
        attn_weights = torch.bmm(q, k.transpose(1, 2))
        if attn_mask is not None:
            attn_weights = torch.nan_to_num(attn_weights)
            attn_weights += attn_mask.unsqueeze(0)
        if key_padding_mask is not None:
            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)
            attn_weights = attn_weights.masked_fill(key_padding_mask.unsqueeze(1).unsqueeze(2).to(torch.bool), float("-inf"))
            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)
        attn_weights = F.softmax(attn_weights, dim=-1, dtype=torch.float32).type_as(attn_weights)
        attn = torch.bmm(attn_weights, v)
        attn = attn.transpose(0, 1).reshape(tgt_len, bsz, embed_dim).transpose(0, 1)
        attn = self.out_proj(attn)
        if need_weights:
            return attn, attn_weights.view(bsz, self.num_heads, tgt_len, src_len).transpose(1, 0)
        return attn, None


def _generate_causal_mask(sz):
    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)
    return mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))


class Beam:
    def __init__(self, char_seq=None, logprobs=None):
        if char_seq is None:
            char_seq = []
        if logprobs is None:
            logprobs = []
        if isinstance(char_seq, list):
            self.chars = torch.tensor(char_seq, dtype=torch.long)
            self.logprobs = torch.tensor(logprobs, dtype=torch.float32)
        else:
            self.chars = char_seq.clone()
            self.logprobs = logprobs.clone()

    def avg_logprob(self):
        return self.logprobs.mean().item()

    def sort_key(self):
        return -self.avg_logprob()

    def seq_end(self, end_tok):
        return self.chars.view(-1)[-1] == end_tok

    def extend(self, idx, logprob):
        return Beam(
            torch.cat([self.chars, idx.unsqueeze(0)], dim=-1),
            torch.cat([self.logprobs, logprob.unsqueeze(0)], dim=-1),
        )


class Hypothesis:
    def __init__(self, device, start_tok, end_tok, padding_tok, memory_idx, num_layers, embd_dim):
        self.device = device
        self.start_tok = start_tok
        self.end_tok = end_tok
        self.padding_tok = padding_tok
        self.memory_idx = memory_idx
        self.embd_size = embd_dim
        self.num_layers = num_layers
        self.cached_activations = [torch.zeros(1, 0, self.embd_size).to(self.device)] * (num_layers + 1)
        self.out_idx = torch.LongTensor([start_tok]).to(self.device)
        self.out_logprobs = torch.FloatTensor([0]).to(self.device)
        self.length = 0

    def seq_end(self):
        return self.out_idx.view(-1)[-1] == self.end_tok

    def logprob(self):
        return self.out_logprobs.mean().item()

    def sort_key(self):
        return -self.logprob()

    def prob(self):
        return self.out_logprobs.mean().exp().item()

    def __len__(self):
        return self.length

    def extend(self, idx, logprob):
        ret = Hypothesis(self.device, self.start_tok, self.end_tok, self.padding_tok,
                         self.memory_idx, self.num_layers, self.embd_size)
        ret.cached_activations = [item.clone() for item in self.cached_activations]
        ret.length = self.length + 1
        ret.out_idx = torch.cat([self.out_idx, torch.LongTensor([idx]).to(self.device)], dim=0)
        ret.out_logprobs = torch.cat([self.out_logprobs, torch.FloatTensor([logprob]).to(self.device)], dim=0)
        return ret

    def output(self):
        return self.cached_activations[-1]


def _next_token_batch(hyps, memory, memory_mask, decoders, embd):
    N = len(hyps)
    offset = len(hyps[0])
    last_toks = torch.stack([item.out_idx[-1] for item in hyps])
    tgt = embd(last_toks).unsqueeze_(1)
    memory_sel = torch.stack([memory[item.memory_idx, :, :] for item in hyps], dim=0)
    for l, layer in enumerate(decoders):
        combined = torch.cat([item.cached_activations[l] for item in hyps], dim=0)
        combined = torch.cat([combined, tgt], dim=1)
        for i in range(N):
            hyps[i].cached_activations[l] = combined[i:i + 1, :, :]
        tgt = tgt + layer.self_attn(layer.norm1(tgt), layer.norm1(combined), layer.norm1(combined), q_offset=offset)[0]
        tgt = tgt + layer.multihead_attn(layer.norm2(tgt), memory_sel, memory_sel, key_padding_mask=memory_mask, q_offset=offset)[0]
        tgt = tgt + layer._ff_block(layer.norm3(tgt))
    for i in range(N):
        hyps[i].cached_activations[len(decoders)] = torch.cat([hyps[i].cached_activations[len(decoders)], tgt[i:i + 1, :, :]], dim=1)
    return tgt.squeeze_(1)


class OCR(nn.Module):
    def __init__(self, dictionary, max_len):
        super().__init__()
        self.max_len = max_len
        self.dictionary = dictionary
        self.dict_size = len(dictionary)
        embd_dim = 320
        nhead = 4
        self.backbone = ConvNext_FeatureExtractor(48, 3, embd_dim)
        self.encoders = nn.ModuleList()
        self.decoders = nn.ModuleList()

        for _ in range(4):
            encoder = nn.TransformerEncoderLayer(embd_dim, nhead, dropout=0, batch_first=True, norm_first=True)
            encoder.self_attn = XposMultiheadAttention(embd_dim, nhead, self_attention=True)
            encoder.forward = _transformer_encoder_forward
            self.encoders.append(encoder)
        self.encoders.forward = self._encoder_forward

        for _ in range(5):
            decoder = nn.TransformerDecoderLayer(embd_dim, nhead, dropout=0, batch_first=True, norm_first=True)
            decoder.self_attn = XposMultiheadAttention(embd_dim, nhead, self_attention=True)
            decoder.multihead_attn = XposMultiheadAttention(embd_dim, nhead, encoder_decoder_attention=True)
            self.decoders.append(decoder)
        self.decoders.forward = self._decoder_forward

        self.embd = nn.Embedding(self.dict_size, embd_dim)
        self.pred1 = nn.Sequential(nn.Linear(embd_dim, embd_dim), nn.GELU(), nn.Dropout(0.15))
        self.pred = nn.Linear(embd_dim, self.dict_size)
        self.pred.weight = self.embd.weight
        self.color_pred1 = nn.Sequential(nn.Linear(embd_dim, 64), nn.ReLU())
        self.color_pred_fg = nn.Linear(64, 3)
        self.color_pred_bg = nn.Linear(64, 3)
        self.color_pred_fg_ind = nn.Linear(64, 2)
        self.color_pred_bg_ind = nn.Linear(64, 2)

    def _encoder_forward(self, memory, encoder_mask):
        for layer in self.encoders:
            memory = layer(layer, src=memory, src_key_padding_mask=encoder_mask)
        return memory

    def _decoder_forward(self, embd_tok, cached_activations, memory, memory_mask, step):
        tgt = embd_tok
        for l, layer in enumerate(self.decoders):
            combined = cached_activations[:, l, :step, :]
            combined = torch.cat([combined, tgt], dim=1)
            cached_activations[:, l, step, :] = tgt.squeeze(1)
            tgt = tgt + layer.self_attn(layer.norm1(tgt), layer.norm1(combined), layer.norm1(combined), q_offset=step)[0]
            tgt = tgt + layer.multihead_attn(layer.norm2(tgt), memory, memory, key_padding_mask=memory_mask, q_offset=step)[0]
            tgt = tgt + layer._ff_block(layer.norm3(tgt))
        cached_activations[:, l + 1, step, :] = tgt.squeeze(1)
        return tgt.squeeze_(1), cached_activations

    def forward(self, img, char_idx, decoder_mask, encoder_mask):
        memory = self.backbone(img)
        memory = einops.rearrange(memory, 'N C 1 W -> N W C')
        for layer in self.encoders:
            memory = layer(memory, src_key_padding_mask=encoder_mask)
        char_embd = self.embd(char_idx)
        N, L = char_idx.shape
        casual_mask = _generate_causal_mask(L).to(img.device)
        decoded = char_embd
        for layer in self.decoders:
            decoded = layer(decoded, memory, tgt_mask=casual_mask, tgt_key_padding_mask=decoder_mask, memory_key_padding_mask=encoder_mask)
        pred_char_logits = self.pred(self.pred1(decoded))
        color_feats = self.color_pred1(decoded)
        return (pred_char_logits, self.color_pred_fg(color_feats), self.color_pred_bg(color_feats),
                self.color_pred_fg_ind(color_feats), self.color_pred_bg_ind(color_feats))

    def infer_beam_batch(self, img, img_widths, beams_k=5, start_tok=1, end_tok=2, pad_tok=0, max_finished_hypos=2, max_seq_length=384):
        N, C, H, W = img.shape
        memory = self.backbone(img)
        memory = einops.rearrange(memory, 'N C 1 W -> N W C')
        valid_feats_length = [(x + 3) // 4 + 2 for x in img_widths]
        input_mask = torch.zeros(N, memory.size(1), dtype=torch.bool).to(img.device)
        for i, l in enumerate(valid_feats_length):
            input_mask[i, l:] = True
        for layer in self.encoders:
            memory = layer(layer, src=memory, src_key_padding_mask=input_mask)
        hypos = [Hypothesis(img.device, start_tok, end_tok, pad_tok, i, len(self.decoders), 320) for i in range(N)]
        decoded = _next_token_batch(hypos, memory, input_mask, self.decoders, self.embd)
        pred_char_logprob = self.pred(self.pred1(decoded)).log_softmax(-1)
        pred_chars_values, pred_chars_index = torch.topk(pred_char_logprob, beams_k, dim=1)
        new_hypos = []
        finished_hypos = defaultdict(list)
        for i in range(N):
            for k in range(beams_k):
                new_hypos.append(hypos[i].extend(pred_chars_index[i, k], pred_chars_values[i, k]))
        hypos = new_hypos
        for _ in range(max_seq_length):
            decoded = _next_token_batch(hypos, memory, torch.stack([input_mask[h.memory_idx] for h in hypos]), self.decoders, self.embd)
            pred_char_logprob = self.pred(self.pred1(decoded)).log_softmax(-1)
            pred_chars_values, pred_chars_index = torch.topk(pred_char_logprob, beams_k, dim=1)
            hypos_per_sample = defaultdict(list)
            for i, h in enumerate(hypos):
                for k in range(beams_k):
                    hypos_per_sample[h.memory_idx].append(h.extend(pred_chars_index[i, k], pred_chars_values[i, k]))
            hypos = []
            for i in hypos_per_sample.keys():
                cur_hypos = sorted(hypos_per_sample[i], key=lambda a: a.sort_key())[:beams_k + 1]
                to_add = []
                sample_done = False
                for h in cur_hypos:
                    if h.seq_end():
                        finished_hypos[i].append(h)
                        if len(finished_hypos[i]) >= max_finished_hypos:
                            sample_done = True
                            break
                    elif len(to_add) < beams_k:
                        to_add.append(h)
                if not sample_done:
                    hypos.extend(to_add)
            if not hypos:
                break
        for i in range(N):
            if i not in finished_hypos:
                cur_hypos = sorted(hypos_per_sample.get(i, []), key=lambda a: a.sort_key())
                if cur_hypos:
                    finished_hypos[i].append(cur_hypos[0])
        result = []
        for i in range(N):
            cur_hypo = sorted(finished_hypos.get(i, []), key=lambda a: a.sort_key())[0]
            decoded_out = cur_hypo.output()
            color_feats = self.color_pred1(decoded_out)
            result.append((
                cur_hypo.out_idx[1:], cur_hypo.prob(),
                self.color_pred_fg(color_feats)[0], self.color_pred_bg(color_feats)[0],
                self.color_pred_fg_ind(color_feats)[0], self.color_pred_bg_ind(color_feats)[0],
            ))
        return result

    def infer_beam_batch_tensor(self, img, img_widths, beams_k=5, start_tok=1, end_tok=2, pad_tok=0, max_finished_hypos=2, max_seq_length=384):
        N, C, H, W = img.shape
        memory = self.backbone(img)
        memory = einops.rearrange(memory, 'N C 1 W -> N W C')
        valid_feats_length = [(x + 3) // 4 + 2 for x in img_widths]
        input_mask = torch.zeros(N, memory.size(1), dtype=torch.bool).to(img.device)
        for i, l in enumerate(valid_feats_length):
            input_mask[i, l:] = True
        memory = self.encoders(memory, input_mask)

        out_idx = torch.full((N, 1), start_tok, dtype=torch.long, device=img.device)
        cached_activations = torch.zeros(N, len(self.decoders) + 1, max_seq_length, 320, device=img.device)
        log_probs = torch.zeros(N, 1, device=img.device)
        idx_embedded = self.embd(out_idx[:, -1:])
        decoded, cached_activations = self.decoders(idx_embedded, cached_activations, memory, input_mask, 0)
        pred_char_logprob = self.pred(self.pred1(decoded)).log_softmax(-1)
        pred_chars_values, pred_chars_index = torch.topk(pred_char_logprob, beams_k, dim=1)

        out_idx = torch.cat([out_idx.unsqueeze(1).expand(-1, beams_k, -1), pred_chars_index.unsqueeze(-1)], dim=-1).reshape(-1, 2)
        log_probs = pred_chars_values.view(-1, 1)
        memory = memory.repeat_interleave(beams_k, dim=0)
        input_mask = input_mask.repeat_interleave(beams_k, dim=0)
        cached_activations = cached_activations.repeat_interleave(beams_k, dim=0)
        batch_index = torch.arange(N).repeat_interleave(beams_k, dim=0).to(img.device)

        finished_hypos = defaultdict(list)
        N_remaining = N

        for step in range(1, max_seq_length):
            idx_embedded = self.embd(out_idx[:, -1:])
            decoded, cached_activations = self.decoders(idx_embedded, cached_activations, memory, input_mask, step)
            pred_char_logprob = self.pred(self.pred1(decoded)).log_softmax(-1)
            pred_chars_values, pred_chars_index = torch.topk(pred_char_logprob, beams_k, dim=1)

            finished = out_idx[:, -1] == end_tok
            pred_chars_values[finished] = 0
            pred_chars_index[finished] = end_tok

            new_out_idx = out_idx.unsqueeze(1).expand(-1, beams_k, -1)
            new_out_idx = torch.cat([new_out_idx, pred_chars_index.unsqueeze(-1)], dim=-1).view(-1, step + 2)
            new_log_probs = (log_probs.unsqueeze(1).expand(-1, beams_k, -1) + pred_chars_values.unsqueeze(-1)).view(-1, 1)

            new_out_idx = new_out_idx.view(N_remaining, -1, step + 2)
            new_log_probs = new_log_probs.view(N_remaining, -1)
            batch_topk_log_probs, batch_topk_indices = new_log_probs.topk(beams_k, dim=1)

            expanded_topk_indices = batch_topk_indices.unsqueeze(-1).expand(-1, -1, new_out_idx.shape[-1])
            out_idx = torch.gather(new_out_idx, 1, expanded_topk_indices).reshape(-1, step + 2)
            log_probs = batch_topk_log_probs.view(-1, 1)

            finished = (out_idx[:, -1] == end_tok).view(N_remaining, beams_k)
            finished_counts = finished.sum(dim=1)
            finished_batch_indices = (finished_counts >= max_finished_hypos).nonzero(as_tuple=False).squeeze()

            if finished_batch_indices.numel() == 0:
                continue
            if finished_batch_indices.dim() == 0:
                finished_batch_indices = finished_batch_indices.unsqueeze(0)

            for idx in finished_batch_indices:
                batch_lp = batch_topk_log_probs[idx]
                best_beam = batch_lp.argmax()
                finished_hypos[batch_index[beams_k * idx].item()] = (
                    out_idx[idx * beams_k + best_beam],
                    torch.exp(batch_lp[best_beam]).item(),
                    cached_activations[idx * beams_k + best_beam],
                )

            remaining = []
            for i in range(N_remaining):
                if i not in finished_batch_indices:
                    for j in range(beams_k):
                        remaining.append(i * beams_k + j)
            if not remaining:
                break
            N_remaining = len(remaining) // beams_k
            ri = torch.tensor(remaining, device=img.device)
            out_idx = out_idx.index_select(0, ri)
            log_probs = log_probs.index_select(0, ri)
            memory = memory.index_select(0, ri)
            cached_activations = cached_activations.index_select(0, ri)
            input_mask = input_mask.index_select(0, ri)
            batch_index = batch_index.index_select(0, ri)

        if len(finished_hypos) < N:
            for i in range(N):
                if i not in finished_hypos:
                    sample_indices = (batch_index == i).nonzero(as_tuple=True)[0]
                    if sample_indices.numel() > 0:
                        bi = sample_indices[0]
                        finished_hypos[i] = (out_idx[bi], torch.exp(log_probs[bi]).item(), cached_activations[bi])
                    else:
                        finished_hypos[i] = (torch.tensor([end_tok], device=img.device), 0.0,
                                             torch.zeros(cached_activations.shape[1:], device=img.device))

        result = []
        for i in range(N):
            final_idx, prob, decoded_cache = finished_hypos[i]
            color_feats = self.color_pred1(decoded_cache[-1].unsqueeze(0))
            result.append((
                final_idx[1:], prob,
                self.color_pred_fg(color_feats)[0], self.color_pred_bg(color_feats)[0],
                self.color_pred_fg_ind(color_feats)[0], self.color_pred_bg_ind(color_feats)[0],
            ))
        return result
